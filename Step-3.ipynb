{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "import timm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "\n",
    "# making sure that the whole embedding tensor is printed in output\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cu102'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! set CUDA_VISIBLE_DEVICES = '0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure the feature extraction runs on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt_file, img_file, img_dir_path, transforms):\n",
    "        self.text_file = txt_file\n",
    "        self.image_file = img_file\n",
    "        self.ingredients = []\n",
    "        self.txt_ids = []\n",
    "        self.img_id_map = {}\n",
    "\n",
    "        self.img_dir = img_dir_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "        for row in self.text_file:\n",
    "            id = row[\"id\"]\n",
    "            self.txt_ids.append(id)\n",
    "            # title = row[\"title\"]\n",
    "            # ingredients = row[\"ingredients\"]\n",
    "            instructions = row[\"instructions\"]\n",
    "\n",
    "            ingredient_text = \"\"\n",
    "            # instructions_text = \"\"\n",
    "\n",
    "            ingredient_text = \" \".join(instruction[\"text\"] for instruction in instructions)\n",
    "\n",
    "            self.ingredients.append(ingredient_text)\n",
    "\n",
    "        for row in self.image_file:\n",
    "            self.img_id_map[row[\"id\"]] = row[\"images\"][0]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ingredients)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.ingredients[idx]\n",
    "        image_file = self.img_id_map[self.txt_ids[idx]]\n",
    "        image_path = self.img_dir + image_file[0] + \"/\" + image_file[1] + \"/\" + image_file[2] + \"/\" + image_file[3] + \"/\" + image_file\n",
    "        img = Image.open(image_path)\n",
    "        img = self.transforms(img)\n",
    "        return img, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming each image\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_dataset = Dataset(json.load(open(\"/common/users/kcm161/data/train/text.json\")), json.load(open(\"/common/users/kcm161/data/train/image.json\")), \\\n",
    "    \"/common/users/kcm161/train/\", data_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "val_dataset = Dataset(json.load(open(\"/common/users/kcm161/data/val/text.json\")), json.load(open(\"/common/users/kcm161/data/val/image.json\")), \\\n",
    "    \"/common/users/kcm161/val/\", data_transforms)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "test_dataset = Dataset(json.load(open(\"/common/users/kcm161/data/test/text.json\")), json.load(open(\"/common/users/kcm161/data/test/image.json\")), \\\n",
    "    \"/common/users/kcm161/test/\", data_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For triplet finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_triplets(images, texts):\n",
    "    triplet_images = torch.zeros((16, 3, 224, 224))\n",
    "    triplet_pos_text = []\n",
    "    triplet_neg_text = []\n",
    "    \n",
    "#     print(images.shape[0])\n",
    "    \n",
    "    for i in range(images.shape[0]):\n",
    "        triplet_images[i] = images[i]\n",
    "        triplet_pos_text.append(texts[i])\n",
    "        \n",
    "#     print(len(triplet_pos_text))\n",
    "        \n",
    "    for i in range(images.shape[0]):\n",
    "        neg_idx = random.randint(0, images.shape[0] - 1)\n",
    "#         print(neg_idx)\n",
    "        while neg_idx == i:\n",
    "            neg_idx = random.randint(0, images.shape[0] - 1)\n",
    "#             print(neg_idx)\n",
    "\n",
    "        triplet_neg_text.append(triplet_pos_text[neg_idx])\n",
    "        \n",
    "    return triplet_images, triplet_pos_text, triplet_neg_text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "for param in img_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in img_model.encoder.layer[11].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in img_model.layernorm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in img_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "img_model = nn.DataParallel(img_model, device_ids = [0])\n",
    "img_model = img_model.to(f'cuda:{img_model.device_ids[0]}')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for param in text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in text_model.encoder.layer[11].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in text_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "text_model = nn.DataParallel(text_model, device_ids = [0])\n",
    "text_model = text_model.to(f'cuda:{text_model.device_ids[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    # Utility function for timers\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_image = torch.optim.Adam(img_model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "optimizer_text = torch.optim.Adam(text_model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "\n",
    "optimizer_total = torch.optim.Adam(list(text_model.parameters()) + list(img_model.parameters()), lr=1e-4, weight_decay=0.0)\n",
    "\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "criterion.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def triplet_train(triplet_loader, img_model, text_model, criterion, optimizer_image, optimizer_text,  optimizer_total, epoch):\n",
    "    print('Starting training epoch {}'.format(epoch))\n",
    "    img_model.train()\n",
    "    text_model.train()\n",
    "    \n",
    "    batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "#     optimizer_image.zero_grad()\n",
    "#     optimizer_text.zero_grad()\n",
    "    optimizer_total.zero_grad()\n",
    "    \n",
    "    train_loss, total_samples, running_loss = 0, 0, 0\n",
    "    \n",
    "    batch = 1\n",
    "    \n",
    "    with tqdm(total = len(triplet_loader)) as pbar:\n",
    "        for img, text in triplet_loader:\n",
    "            \n",
    "            img, pos_text, neg_text = generate_triplets(img, text)\n",
    "\n",
    "            image_encodings = img_model(img.to(f'cuda:{text_model.device_ids[0]}'))\n",
    "            \n",
    "            pos_encoded_ingredients = tokenizer(pos_text, return_tensors='pt', max_length=512, truncation = True, padding = True).to(f'cuda:{text_model.device_ids[0]}')\n",
    "            pos_output_ingredients = text_model(**pos_encoded_ingredients)\n",
    "\n",
    "            neg_encoded_ingredients = tokenizer(neg_text, return_tensors='pt', max_length=512, truncation = True, padding = True).to(f'cuda:{text_model.device_ids[0]}')\n",
    "            neg_output_ingredients = text_model(**neg_encoded_ingredients)\n",
    "\n",
    "            loss = criterion(image_encodings[\"last_hidden_state\"][:, 0, :], pos_output_ingredients[\"last_hidden_state\"][:, 0, :], \n",
    "                             neg_output_ingredients[\"last_hidden_state\"][:, 0, :])\n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            # Compute gradient and optimize\n",
    "#             optimizer_image.zero_grad()\n",
    "#             optimizer_text.zero_grad()\n",
    "            optimizer_total.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "#             optimizer_image.step()\n",
    "#             optimizer_text.step()\n",
    "            optimizer_total.step()\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            running_loss += loss.item() * img.shape[0]\n",
    "            total_samples += img.shape[0]\n",
    "\n",
    "            train_loss += running_loss\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print('  batch {} loss: {}'.format(batch, running_loss / 50))\n",
    "                running_loss = 0.\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val} ({batch_time.avg})\\t'\n",
    "                    'Data {data_time.val} ({data_time.avg})\\t'.format(\n",
    "                      epoch, batch, len(train_loader), batch_time=batch_time,\n",
    "                     data_time=data_time)) \n",
    "                pbar.update(50)\n",
    "\n",
    "            batch += 1\n",
    "\n",
    "        print('Finished training epoch {}'.format(epoch))\n",
    "        print('Epoch Loss:', train_loss / total_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = {\n",
    "    \"image_vit_encoder\": img_model.state_dict(),\n",
    "    \"text_encoder\": text_model.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(save_dict, f'/common/users/kcm161/step3_models_instructions_onlytriplet_e1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_weights = torch.load(\"/common/users/kcm161/step3_models_onlytriplet_e1.pt\", map_location = \"cuda:0\")\n",
    "\n",
    "img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "for param in img_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in img_model.encoder.layer[11].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in img_model.layernorm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in img_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "img_model = nn.DataParallel(img_model, device_ids = [0])\n",
    "img_model.load_state_dict(model_weights[\"image_vit_encoder\"])\n",
    "img_model = img_model.to(f'cuda:{img_model.device_ids[0]}')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for param in text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in text_model.encoder.layer[11].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in text_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "text_model = nn.DataParallel(text_model, device_ids = [0])\n",
    "text_model.load_state_dict(model_weights[\"text_encoder\"])\n",
    "text_model = text_model.to(f'cuda:{text_model.device_ids[0]}')\n",
    "\n",
    "optimizer_image = torch.optim.Adam(img_model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "optimizer_text = torch.optim.Adam(text_model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "\n",
    "optimizer_total = torch.optim.Adam(list(text_model.parameters()) + list(img_model.parameters()), lr=1e-4, weight_decay=0.0)\n",
    "\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "criterion.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    triplet_train(triplet_train_loader, img_model, text_model, criterion, optimizer_image, optimizer_text, optimizer_total, epoch)\n",
    "    \n",
    "    save_dict = {\n",
    "        \"image_vit_encoder\": img_model.state_dict(),\n",
    "        \"text_encoder\": text_model.state_dict(),\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, f'/common/users/kcm161/step3_models_instructions_onlytriplet_e{epoch}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Embeddings - ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in img_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in img_model.encoder.layer[11].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in img_model.layernorm.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in img_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "img_model = nn.DataParallel(img_model, device_ids = [0])\n",
    "img_model = img_model.to(f'cuda:{img_model.device_ids[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embeddings - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for param in text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in text_model.encoder.layer[11].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in text_model.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "text_model = nn.DataParallel(text_model, device_ids = [0])\n",
    "text_model = text_model.to(f'cuda:{text_model.device_ids[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ViT and BERT weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_weights = torch.load(\"/common/users/kcm161/step3_models_instructions_onlytriplet_e1.pt\", map_location = \"cuda:0\")\n",
    "\n",
    "img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "for param in img_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "img_model = nn.DataParallel(img_model, device_ids = [0])\n",
    "img_model.load_state_dict(model_weights[\"image_vit_encoder\"])\n",
    "img_model = img_model.to(f'cuda:{img_model.device_ids[0]}')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "for param in text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "text_model = nn.DataParallel(text_model, device_ids = [0])\n",
    "text_model.load_state_dict(model_weights[\"text_encoder\"])\n",
    "text_model = text_model.to(f'cuda:{text_model.device_ids[0]}')\n",
    "\n",
    "optimizer_image = torch.optim.Adam(img_model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "optimizer_text = torch.optim.Adam(text_model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "\n",
    "optimizer_total = torch.optim.Adam(list(text_model.parameters()) + list(img_model.parameters()), lr=1e-4, weight_decay=0.0)\n",
    "\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "criterion.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout( p = dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe = torch.transpose(pe, 0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[0, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, model_dim = 768, n_heads = 2, n_layers = 2, num_image_patches = 197, num_classes = 2, drop_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.text_positional = SinusoidalPositionalEncoding(model_dim, dropout= drop_rate)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        self.sep_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        self.image_positional = nn.Parameter(torch.zeros(1, num_image_patches + 1, model_dim))\n",
    "        self.image_positional_drop = nn.Dropout(p = drop_rate)\n",
    "        layers = nn.TransformerEncoderLayer(d_model = model_dim, nhead = n_heads, batch_first= True)\n",
    "        self.encoder = nn.TransformerEncoder(layers, num_layers = n_layers)\n",
    "        self.cls_projection = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, image_features, text_features, src_key_padding_mask = None):\n",
    "        # image_features = image_features.to(device)\n",
    "\n",
    "        image_features *= math.sqrt(768)\n",
    "        text_features *= math.sqrt(768)\n",
    "\n",
    "        batch_size = image_features.shape[0]\n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        image_features = torch.cat((cls_token, image_features), dim = 1)\n",
    "        image_features = image_features + self.image_positional\n",
    "        image_features = self.image_positional_drop(image_features)\n",
    "\n",
    "        text_features = self.text_positional(text_features)\n",
    "        sep_token = self.sep_token.expand(batch_size, -1, -1)\n",
    "\n",
    "        transformer_input = torch.cat((image_features, sep_token, text_features), dim = 1)\n",
    "        if src_key_padding_mask is not None:\n",
    "            src_key_padding_mask = torch.cat((torch.zeros(image_features.shape[0], image_features.shape[1] + 1).to(f'cuda:{transformer.device_ids[0]}'), \n",
    "                                              src_key_padding_mask.to(f'cuda:{transformer.device_ids[0]}')), dim  = 1)\n",
    "\n",
    "        transformer_outputs = self.encoder(transformer_input, src_key_padding_mask = src_key_padding_mask)\n",
    "        projected_output = transformer_outputs[:, 0, :]\n",
    "        \n",
    "        return self.cls_projection(projected_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = CrossModalAttention()\n",
    "transformer = nn.DataParallel(transformer, device_ids=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    # Utility function for timers\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_image = torch.optim.SGD(img_model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "optimizer_text = torch.optim.SGD(text_model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "optimizer_transformer = torch.optim.SGD(transformer.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "img_model.to(device);\n",
    "text_model.to(device);\n",
    "transformer.to(device);\n",
    "criterion.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(train_loader, img_model, text_model, transformer, criterion, optimizer_image, optimizer_text, optimizer_transformer, epoch):\n",
    "    print('Starting training epoch {}'.format(epoch))\n",
    "    img_model.train()\n",
    "    text_model.train()\n",
    "    transformer.train()\n",
    "    \n",
    "    batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    optimizer_image.zero_grad()\n",
    "    optimizer_text.zero_grad()\n",
    "    optimizer_transformer.zero_grad()\n",
    "    \n",
    "    train_loss, total_samples, running_loss = 0, 0, 0\n",
    "    \n",
    "    batch = 1\n",
    "    \n",
    "    with tqdm(total = len(train_loader)) as pbar:\n",
    "        for img, text in train_loader:\n",
    "        \n",
    "            # Run forward pass\n",
    "    #         print(img.shape)\n",
    "            image_encodings = img_model(img.to(f'cuda:{text_model.device_ids[0]}'))\n",
    "\n",
    "            encoded_ingredients = tokenizer(text, return_tensors='pt', max_length=512, truncation = True, padding = True).to(f'cuda:{text_model.device_ids[0]}')\n",
    "            output_ingredients = text_model(**encoded_ingredients)\n",
    "\n",
    "            transformer_image_inputs, transformer_text_inputs, output_attention_mask, ground_truth = get_transformer_input (image_encodings[\"last_hidden_state\"], \n",
    "                                                                                                                            output_ingredients[\"last_hidden_state\"], \n",
    "                                                                                                                            encoded_ingredients.attention_mask)\n",
    "            text_padding_mask = ~output_attention_mask.bool()\n",
    "            outputs = transformer(transformer_image_inputs.to(f'cuda:{transformer.device_ids[0]}'), transformer_text_inputs.to(f'cuda:{transformer.device_ids[0]}'), \n",
    "                                  text_padding_mask.to(f'cuda:{transformer.device_ids[0]}'))\n",
    "\n",
    "            loss = criterion(outputs, ground_truth.to(f'cuda:{transformer.device_ids[0]}')) \n",
    "\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            # Compute gradient and optimize\n",
    "            optimizer_image.zero_grad()\n",
    "            optimizer_text.zero_grad()\n",
    "            optimizer_transformer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_image.step()\n",
    "            optimizer_text.step()\n",
    "            optimizer_transformer.step()\n",
    "\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            running_loss += loss.item() * img.shape[0]\n",
    "            total_samples += img.shape[0]\n",
    "\n",
    "            train_loss += running_loss\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print('  batch {} loss: {}'.format(batch, running_loss / 50))\n",
    "                running_loss = 0.\n",
    "\n",
    "            if batch % 50 == 0:\n",
    "                print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                    'Time {batch_time.val} ({batch_time.avg})\\t'\n",
    "                    'Data {data_time.val} ({data_time.avg})\\t'.format(\n",
    "                      epoch, batch, len(train_loader), batch_time=batch_time,\n",
    "                     data_time=data_time)) \n",
    "                pbar.update(50)\n",
    "\n",
    "            batch += 1\n",
    "\n",
    "        print('Finished training epoch {}'.format(epoch))\n",
    "        print('Epoch Loss:', train_loss / total_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_input(image_features, text_features, input_attention_mask):\n",
    "    neg_to_pos_ratio = 2\n",
    "    \n",
    "    input_batch_size = image_features.shape[0]\n",
    "    output_batch_size = (neg_to_pos_ratio + 1) * input_batch_size\n",
    "    ground_truths = torch.zeros(output_batch_size)\n",
    "    ground_truths[:input_batch_size] = 1\n",
    "        \n",
    "    final_image_features = torch.zeros(output_batch_size, *image_features.shape[1:])\n",
    "    final_text_features = torch.zeros(output_batch_size, *text_features.shape[1:])\n",
    "    output_attention_mask = torch.zeros(output_batch_size, *input_attention_mask.shape[1:])\n",
    "    \n",
    "    final_image_features[: input_batch_size] = image_features\n",
    "    final_text_features[:input_batch_size] = text_features\n",
    "    output_attention_mask[:input_batch_size] = input_attention_mask\n",
    "    \n",
    "    for n in range(neg_to_pos_ratio):\n",
    "        a = torch.randperm(input_batch_size)\n",
    "        b = torch.zeros(input_batch_size).to(dtype = torch.int64)\n",
    "        \n",
    "        for idx in range(input_batch_size):\n",
    "            c = random.randint(0, input_batch_size - 1)\n",
    "            while c == a[idx]:\n",
    "                c = random.randint(0, input_batch_size - 1)\n",
    "            \n",
    "            b[idx] = c\n",
    "            \n",
    "        final_image_features[((1 + n) * input_batch_size): (2 + n) * input_batch_size] = image_features[a]\n",
    "        final_text_features[(1 + n) * input_batch_size: (2 + n) * input_batch_size] = text_features[b]\n",
    "        output_attention_mask[(1 + n) * input_batch_size: (2 + n) * input_batch_size] = input_attention_mask[b]\n",
    "        \n",
    "    return final_image_features, final_text_features, output_attention_mask, ground_truths.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    train(train_loader, img_model, text_model, transformer, criterion, optimizer_image, optimizer_text, optimizer_transformer, epoch)\n",
    "    \n",
    "    save_dict = {\n",
    "        \"image_vit_encoder\": img_model.state_dict(),\n",
    "        \"text_encoder\": text_model.state_dict(),\n",
    "        \"cm_transformer\": transformer.state_dict()\n",
    "    }\n",
    "    \n",
    "    torch.save(save_dict, f'/common/users/kcm161/step3_models_instructions_with_triplet_sgd_e{epoch}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = {\n",
    "    \"image_vit_encoder\": img_model.state_dict(),\n",
    "    \"text_encoder\": text_model.state_dict(),\n",
    "    \"cm_transformer\": transformer.state_dict()\n",
    "}\n",
    "\n",
    "torch.save(save_dict, f'/common/users/kcm161/step3_models_with_triplet_e1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_weights = torch.load(\"/common/users/kcm161/step3_models_with_triplet_e1.pt\", map_location = torch.device(\"cuda:1\"))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "for param in text_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in img_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "transformer = CrossModalAttention()\n",
    "\n",
    "optimizer_image = torch.optim.SGD(img_model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "optimizer_text = torch.optim.SGD(text_model.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "optimizer_transformer = torch.optim.SGD(transformer.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "text_model = nn.DataParallel(text_model, device_ids=[1])\n",
    "text_model.load_state_dict(model_weights[\"text_encoder\"])\n",
    "text_model.to((f'cuda:{text_model.device_ids[0]}'));\n",
    "text_model.train();\n",
    "\n",
    "img_model = nn.DataParallel(img_model, device_ids=[1])\n",
    "img_model.load_state_dict(model_weights[\"image_vit_encoder\"])\n",
    "img_model.to((f'cuda:{img_model.device_ids[0]}'));\n",
    "img_model.train();\n",
    "\n",
    "transformer = nn.DataParallel(transformer, device_ids=[1])\n",
    "# transformer.load_state_dict(model_weights_1[\"cm_transformer\"])\n",
    "transformer.to((f'cuda:{transformer.device_ids[0]}'));\n",
    "transformer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60740"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate validation and test embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "transformer = CrossModalAttention()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model_weights = torch.load(\"/common/users/kcm161/step3_models_with_triplet_sgd_e1.pt\", map_location=\"cuda:1\")\n",
    "\n",
    "text_model = nn.DataParallel(text_model, device_ids=[1])\n",
    "text_model.load_state_dict(model_weights[\"text_encoder\"])\n",
    "text_model.to((f'cuda:{text_model.device_ids[0]}'));\n",
    "text_model.eval();\n",
    "\n",
    "img_model = nn.DataParallel(img_model, device_ids=[1])\n",
    "img_model.load_state_dict(model_weights[\"image_vit_encoder\"])\n",
    "img_model.to((f'cuda:{img_model.device_ids[0]}'));\n",
    "img_model.eval();\n",
    "\n",
    "transformer = nn.DataParallel(transformer, device_ids=[1])\n",
    "transformer.load_state_dict(model_weights[\"cm_transformer\"])\n",
    "transformer.to((f'cuda:{transformer.device_ids[0]}'));\n",
    "transformer.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(loader, img_model, text_model, transformer):\n",
    "    \n",
    "    model_weights = torch.load(\"/common/users/kcm161/step3_models_instructions_with_triplet_sgd_e2.pt\", map_location=\"cuda:0\")\n",
    "\n",
    "    text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    img_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "    transformer = CrossModalAttention()\n",
    "\n",
    "    text_model = nn.DataParallel(text_model, device_ids=[0])\n",
    "    text_model.load_state_dict(model_weights[\"text_encoder\"])\n",
    "    text_model.to((f'cuda:{text_model.device_ids[0]}'));\n",
    "    text_model.eval();\n",
    "\n",
    "    img_model = nn.DataParallel(img_model, device_ids=[0])\n",
    "    img_model.load_state_dict(model_weights[\"image_vit_encoder\"])\n",
    "    img_model.to((f'cuda:{img_model.device_ids[0]}'));\n",
    "    img_model.eval();\n",
    "\n",
    "    transformer = nn.DataParallel(transformer, device_ids=[0])\n",
    "    transformer.load_state_dict(model_weights[\"cm_transformer\"])\n",
    "    transformer.to((f'cuda:{transformer.device_ids[0]}'));\n",
    "    transformer.eval();\n",
    "        \n",
    "    img_encodings = np.zeros((len(loader),), dtype = object)\n",
    "    text_encodings = np.zeros((len(loader),), dtype = object)\n",
    "    text_masks = np.zeros((len(loader),), dtype = object)\n",
    "    \n",
    "    idx = 0\n",
    "\n",
    "    print(img_encodings.shape, len(loader))\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for img, text in loader:\n",
    "\n",
    "            # Run forward pass\n",
    "            image_output = img_model(img.to(f'cuda:{img_model.device_ids[0]}'))\n",
    "            encoded_ingredients = tokenizer(text, return_tensors='pt', max_length=512, truncation = True, padding = True).to(f'cuda:{text_model.device_ids[0]}')\n",
    "            output_ingredients = text_model(**encoded_ingredients)\n",
    "            input_attention_mask = encoded_ingredients.attention_mask\n",
    "            text_padding_mask = ~input_attention_mask.bool()\n",
    "\n",
    "            img_encodings[idx] = image_output[\"last_hidden_state\"].cpu().detach().numpy()\n",
    "            text_encodings[idx] = output_ingredients[\"last_hidden_state\"].cpu().detach().numpy()\n",
    "            text_masks[idx] = text_padding_mask.cpu().detach().numpy()\n",
    "        \n",
    "            idx += 1\n",
    "\n",
    "            if idx % 2000 == 0:\n",
    "                print(idx)\n",
    "                # print(img_encodings[idx-1])\n",
    "    # print(img_encodings)\n",
    "\n",
    "    return img_encodings, text_encodings, text_masks\n",
    "    \n",
    "    # ranker(img_encodings, text_encodings, text_masks, transformer, \"recipe\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking extractor embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean median 15.7\n",
      "Recall {1: 0.09549999999999999, 5: 0.28869999999999996, 10: 0.4130999999999999}\n"
     ]
    }
   ],
   "source": [
    "triplet_ranker(img_encode, text_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_ranker(im_vecs, instr_vecs, N = 1000, flag = \"image\"):\n",
    "    # Ranker\n",
    "    idxs = range(N)\n",
    "\n",
    "    glob_rank = []\n",
    "    glob_recall = {1:0.0,5:0.0,10:0.0}\n",
    "    for i in range(10):\n",
    "\n",
    "        ids = random.sample(range(0,len(im_vecs)), N)\n",
    "        \n",
    "        im_sub = im_vecs[ids]\n",
    "        instr_sub = instr_vecs[ids]\n",
    "\n",
    "        if flag == \"image\":\n",
    "            sims = np.dot(im_sub,instr_sub.reshape((N, 768, 1))) # for im2recipe\n",
    "        else:\n",
    "            sims = np.dot(instr_sub,im_sub.T) # for recipe2im\n",
    "\n",
    "        med_rank = []\n",
    "        recall = {1:0.0,5:0.0,10:0.0}\n",
    "\n",
    "        sims = sims.squeeze(1).squeeze(2)\n",
    "\n",
    "        for ii in idxs:\n",
    "\n",
    "            # name = ids_sub[ii]\n",
    "            # get a column of similarities\n",
    "            sim = sims[ii]\n",
    "\n",
    "            # sort indices in descending order\n",
    "            sorting = np.argsort(sim)[::-1].tolist()\n",
    "\n",
    "            # print(sorting)\n",
    "\n",
    "            # find where the index of the pair sample ended up in the sorting\n",
    "            pos = sorting.index(ii)\n",
    "\n",
    "            if (pos+1) == 1:\n",
    "                recall[1]+=1\n",
    "            if (pos+1) <=5:\n",
    "                recall[5]+=1\n",
    "            if (pos+1)<=10:\n",
    "                recall[10]+=1\n",
    "\n",
    "            # store the position\n",
    "            med_rank.append(pos+1)\n",
    "\n",
    "        for i in recall.keys():\n",
    "            recall[i]=recall[i]/N\n",
    "\n",
    "        med = np.median(med_rank)\n",
    "#         print (\"median\", med)\n",
    "\n",
    "        for i in recall.keys():\n",
    "            glob_recall[i]+=recall[i]\n",
    "        glob_rank.append(med)\n",
    "\n",
    "    for i in glob_recall.keys():\n",
    "        glob_recall[i] = glob_recall[i]/10\n",
    "    \n",
    "    print (\"Mean median\", np.average(glob_rank))\n",
    "    print (\"Recall\", glob_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Transformer outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients - testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encodings, text_encodings, text_masks = generate_embeddings(test_loader, img_model, text_model, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.8, 1.5362291495737217, {1: 0.072, 5: 0.286, 10: 0.487})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ingredients im2recipe\n",
    "ranker(img_encodings, text_encodings, text_masks, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.1,\n",
       " 1.2206555615733703,\n",
       " {1: 0.159, 5: 0.4699999999999999, 10: 0.6340000000000001})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ingredients recipe2im\n",
    "ranker(img_encodings, text_encodings, text_masks, transformer, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title - testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encodings, text_encodings, text_masks = generate_embeddings(test_loader, img_model, text_model, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.5,\n",
       " 3.0413812651491097,\n",
       " {1: 0.046000000000000006, 5: 0.15599999999999997, 10: 0.26200000000000007})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# title im2recipe\n",
    "ranker(img_encodings, text_encodings, text_masks, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27.1,\n",
       " 2.981610303175115,\n",
       " {1: 0.033999999999999996, 5: 0.14200000000000002, 10: 0.24300000000000002})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# title im2recipe\n",
    "ranker(img_encodings, text_encodings, text_masks, transformer, \"recipe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions - testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Text - testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranker(img_encodings, text_encodings, text_masks, transformer, retrieval_type = \"recipe\", n = 100):\n",
    "    t = time.time()\n",
    "    data_size = len(img_encodings)\n",
    "    \n",
    "    glob_rank = []\n",
    "    glob_recall = {1:0.0, 5:0.0, 10:0.0}\n",
    "    \n",
    "    # with tqdm (total = n * n * 10) as pbar:\n",
    "    for i in range(10):\n",
    "        ids_sub = np.random.choice(data_size, n, replace = False)\n",
    "        # imgs_sub = img_encodings[ids_sub, :] # numpy \n",
    "        # text_sub = text_encodings[ids_sub, :] # numpy\n",
    "        # attn_sub = text_masks[ids_sub, :] # numpy\n",
    "\n",
    "        imgs_sub = img_encodings[ids_sub] # numpy \n",
    "        text_sub = text_encodings[ids_sub] # numpy\n",
    "        attn_sub = text_masks[ids_sub] # numpy\n",
    "                \n",
    "        probs = torch.zeros((n,n)).detach().cpu()\n",
    "        \n",
    "        if retrieval_type == \"recipe\":\n",
    "            for x in range(n):\n",
    "                for y in range(n):\n",
    "                    temp = transformer(torch.from_numpy(imgs_sub[x]), torch.from_numpy(text_sub[y]), torch.from_numpy(attn_sub[y]))\n",
    "                    probs[x, y] =  nn.Softmax(dim=1)(temp)[0][1].detach().cpu()\n",
    "                    # print(probs[x,y])\n",
    "                \n",
    "                # pbar.update(n)\n",
    "\n",
    "        else:\n",
    "            for x in range(n):\n",
    "                for y in range(n):\n",
    "                    temp = transformer(torch.from_numpy(imgs_sub[y]), torch.from_numpy(text_sub[x]), torch.from_numpy(attn_sub[x]))\n",
    "                    probs[x, y] =  nn.Softmax(dim=1)(temp)[0][1].detach().cpu()\n",
    "                    # print(probs[x,y])\n",
    "                \n",
    "\n",
    "        ranks, _ = compute_ranks(probs.numpy())\n",
    "        \n",
    "        recall = {1: 0.0, 5:0.0, 10:0.0}\n",
    "        for ii in recall.keys():\n",
    "            recall[ii] = (ranks <= ii).sum() / ranks.shape[0]\n",
    "        med = int(np.median(ranks))\n",
    "        # print(med, recall)\n",
    "        for ii in recall.keys():\n",
    "            glob_recall[ii] += recall[ii]\n",
    "        glob_rank.append(med)\n",
    "\n",
    "        # print(i)\n",
    "        \n",
    "    for i in glob_recall.keys():\n",
    "        glob_recall[i] /= 10\n",
    "        \n",
    "    medR = np.mean(glob_rank)\n",
    "    medR_std = np.std(glob_rank)\n",
    "            \n",
    "    return medR, medR_std, glob_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ranks(sims):\n",
    "    ranks = []\n",
    "    preds = []\n",
    "\n",
    "    # print(sims, sims.shape)\n",
    "    \n",
    "    for ii in range(sims.shape[0]):\n",
    "        sim = sims[ii, :]\n",
    "        sorting = np.argsort(sim)[::-1].tolist()\n",
    "        pos = sorting.index(ii)\n",
    "        \n",
    "        ranks.append(pos + 1.0)\n",
    "        preds.append(sorting[0])\n",
    "        \n",
    "    return np.asarray(ranks), preds"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba9bc282ea7dd8acf6b93a88ab047ea17bba2d98cff2c21ca6cffa26ac4d8f39"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
